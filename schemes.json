{
  "_meta": {
    "version": "1.2.1",
    "updated": "2025-09-07",
    "notes": "Removes A/G and {B,C} placeholders; one-liners are generic and self-contained."
  },
  "practical_reasoning": {
    "name": "Argument from Practical Reasoning",
    "description": "From a problem/goal to a recommended action or policy.",
    "critical_questions": [
      {
        "id": "achieves",
        "title": "Effectiveness",
        "question": "Does the recommended option plausibly achieve the stated goal?",
        "short": "Explain how the option is expected to achieve the stated goal; cite outcome evidence.",
        "why_it_matters": "Policies should be selected for expected impact on the stated goal, not just intent.",
        "how_to_answer": [
          "Summarize the pathway from the option to the goal and cite one outcome evaluation.",
          "Describe the mechanism/pathway to impact.",
          "Quantify expected effect size with uncertainty, if possible."
        ],
        "evidence_types": [
          "Program evaluations",
          "RCTs/Quasi-experiments",
          "Meta-analyses",
          "Administrative data",
          "Credible models"
        ],
        "answer_templates": [
          "Evidence indicates the option improves the primary outcome under comparable conditions."
        ],
        "pitfalls": [
          "Describing intent instead of effect",
          "Appeal to authority without outcome data"
        ]
      },
      {
        "id": "alternatives",
        "title": "Alternatives",
        "question": "Is the recommended option preferable to realistic alternatives?",
        "short": "Compare the option with realistic alternatives on impact, cost, speed; justify the choice.",
        "why_it_matters": "Decision quality depends on comparative, not absolute, reasoning.",
        "how_to_answer": [
          "List two realistic alternatives and compare them on impact, cost, and time-to-effect.",
          "List 2–3 viable alternatives.",
          "Explain why the recommended option is preferable under constraints."
        ],
        "evidence_types": [
          "Comparative evaluations",
          "Cost-effectiveness analyses",
          "Opportunity cost estimates"
        ],
        "answer_templates": [
          "Compared with the main alternatives, the recommended option offers superior net benefits under the constraints."
        ],
        "pitfalls": [
          "Straw-manning alternatives",
          "Ignoring feasibility constraints"
        ]
      },
      {
        "id": "side_effects",
        "title": "Side effects",
        "question": "What costs/harms accompany the option, and how are they mitigated?",
        "short": "Identify major risks and costs and how they’re mitigated; note residual risk.",
        "why_it_matters": "Unaddressed harms can outweigh benefits or create new problems.",
        "how_to_answer": [
          "List the main risks/costs and the mitigation plan; describe residual risk after mitigation.",
          "List major risks and affected groups.",
          "Describe mitigations and residual risk.",
          "Explain monitoring for unintended effects."
        ],
        "evidence_types": [
          "Risk assessments",
          "Historical case studies",
          "Equity impact analyses"
        ],
        "answer_templates": [
          "Risks and costs are identified, mitigated, and monitored; residual risk is acceptable."
        ],
        "pitfalls": [
          "Hand-waving away costs",
          "No owner for mitigation"
        ]
      },
      {
        "id": "conflicts",
        "title": "Goal conflicts",
        "question": "Does the option undermine other important goals or constraints?",
        "short": "Show alignment with adjacent goals or constraints, or justify acceptable trade‑offs.",
        "why_it_matters": "Policies interact; gains on one goal may erode others unless balanced.",
        "how_to_answer": [
          "List adjacent goals/constraints and show alignment or justify the trade‑off and mitigation.",
          "List adjacent goals/constraints (budget, legal, equity).",
          "Explain compensating measures or thresholds."
        ],
        "evidence_types": [
          "Budget/legal analyses",
          "MCDA",
          "Stakeholder commitments"
        ],
        "answer_templates": [
          "The option aligns with key constraints; where trade‑offs exist, they are justified and mitigated."
        ],
        "pitfalls": [
          "Ignoring statutory/budget limits",
          "Vague 'we'll manage it' claims"
        ]
      },
      {
        "id": "preconditions",
        "title": "Feasibility",
        "question": "Are resources, authority, capacity, and timing in place?",
        "short": "Confirm feasibility: budget, staff, authority, timing.",
        "why_it_matters": "A good plan without capability fails in execution.",
        "how_to_answer": [
          "Confirm that the practical preconditions are satisfied or will be in time.",
          "Confirm budget, staffing, legal authority, data systems, and timeline.",
          "Name owners and readiness milestones."
        ],
        "evidence_types": [
          "Implementation plans",
          "Authorizations",
          "Hiring/procurement timelines"
        ],
        "answer_templates": [
          "Feasibility conditions are met or on track; critical path risks are owned."
        ],
        "pitfalls": [
          "Assuming capacity exists",
          "Undefined critical path"
        ]
      }
    ]
  },
  "argument_from_consequences": {
    "name": "Argument from Consequences",
    "description": "Recommend or oppose an option because of expected consequences.",
    "critical_questions": [
      {
        "id": "probability",
        "title": "Likelihood",
        "question": "How likely are the predicted outcomes?",
        "short": "Provide probability ranges or scenario weights with justification.",
        "why_it_matters": "Overconfident claims mislead decisions under uncertainty.",
        "how_to_answer": [
          "Provide a justified probability range or scenario breakdown for the outcomes.",
          "State key assumptions and sensitivity.",
          "Quantify uncertainty rather than a single point estimate."
        ],
        "evidence_types": [
          "Forecasts",
          "Historical frequencies",
          "Ensemble models"
        ],
        "answer_templates": [
          "Probability estimates are justified and sensitivity has been explored."
        ],
        "pitfalls": [
          "Point estimates without uncertainty",
          "Ignoring assumption risk"
        ]
      },
      {
        "id": "severity",
        "title": "Severity",
        "question": "How important or serious are the outcomes?",
        "short": "Quantify stakes with decision‑relevant metrics (health, cost, equity).",
        "why_it_matters": "Low‑probability but severe outcomes may dominate decisions.",
        "how_to_answer": [
          "Quantify stakes using relevant metrics and compare to thresholds.",
          "Select decision‑relevant metrics and quantify magnitude.",
          "Compare to thresholds or benchmarks."
        ],
        "evidence_types": [
          "Impact assessments",
          "Cost/benefit studies",
          "Equity analyses"
        ],
        "answer_templates": [
          "The magnitude of impact is quantified and decision thresholds are considered."
        ],
        "pitfalls": [
          "Qualitative adjectives without numbers",
          "Cherry‑picked metrics"
        ]
      },
      {
        "id": "countervailing",
        "title": "Countervailing effects",
        "question": "What offsetting consequences exist?",
        "short": "Identify offsetting consequences and net them out.",
        "why_it_matters": "Net impact depends on both intended and opposing effects.",
        "how_to_answer": [
          "List opposing pathways and estimate their size; report the net.",
          "Identify opposing effects and their magnitudes.",
          "Explain why the net remains favorable (or not)."
        ],
        "evidence_types": [
          "System models",
          "Before/after analyses"
        ],
        "answer_templates": [
          "Opposing effects have been accounted for; net impact remains favorable (or not)."
        ],
        "pitfalls": [
          "Ignoring rebound/displacement",
          "Double-counting benefits"
        ]
      }
    ]
  },
  "rules_to_case": {
    "name": "Apply Rule to Case",
    "description": "Infer a conclusion by applying a general rule to a specific case.",
    "critical_questions": [
      {
        "id": "matching",
        "title": "Rule matching",
        "question": "Does the case meet the rule’s stated conditions?",
        "short": "Map each rule condition to the corresponding facts of the case.",
        "why_it_matters": "Misapplied rules yield unsound conclusions.",
        "how_to_answer": [
          "List the rule’s conditions and point to the facts that satisfy each; address ambiguities.",
          "List the rule conditions and matching facts.",
          "Explain treatment of any ambiguous or missing conditions."
        ],
        "evidence_types": [
          "Statutes/policies",
          "Case facts",
          "Official records"
        ],
        "answer_templates": [
          "All rule conditions are satisfied by the cited facts; interpretations are justified."
        ],
        "pitfalls": [
          "Asserting applicability without mapping facts",
          "Ignoring exceptions"
        ]
      },
      {
        "id": "exceptions",
        "title": "Exceptions",
        "question": "Do any rule‑exceptions apply?",
        "short": "Name possible exceptions and why they do not apply or are overridden.",
        "why_it_matters": "Rules often have codified exceptions; overlooking them weakens the inference.",
        "how_to_answer": [
          "List candidate exceptions and justify non‑applicability or legitimate override.",
          "List plausible exceptions and test applicability.",
          "If applicable, justify override/waiver and authority."
        ],
        "evidence_types": [
          "Rule exception clauses",
          "Precedents",
          "Waiver authorities"
        ],
        "answer_templates": [
          "No applicable exception remains, or override is justified and authorized."
        ],
        "pitfalls": [
          "Silence on known exceptions",
          "Unauthorized overrides"
        ]
      }
    ]
  },
  "definition": {
    "name": "Definition to Case",
    "description": "Classify a case under a definition with explicit criteria.",
    "critical_questions": [
      {
        "id": "fit",
        "title": "Definition fit",
        "question": "Do the case facts satisfy each definition criterion?",
        "short": "Map definition criteria to the facts and address borderline status.",
        "why_it_matters": "Misclassification stems from ignoring criteria or ambiguity.",
        "how_to_answer": [
          "List the definition criteria and show the corresponding facts; handle borderline cases.",
          "List criteria and facts.",
          "Discuss vagueness and interpretation principles for close cases."
        ],
        "evidence_types": [
          "Standards",
          "Taxonomies",
          "Fact records"
        ],
        "answer_templates": [
          "Each criterion is satisfied (or not) with explicit reasoning for gray areas."
        ],
        "pitfalls": [
          "Skipping criteria",
          "Hand‑waving borderline cases"
        ]
      },
      {
        "id": "adequacy",
        "title": "Definition adequacy",
        "question": "Is this definition accepted and appropriate in this context?",
        "short": "Show the chosen definition is appropriate here.",
        "why_it_matters": "Competing definitions exist; choice affects outcome.",
        "how_to_answer": [
          "Cite authority/consensus for this definition and explain why alternatives are inferior for this use.",
          "Cite authority or consensus in context.",
          "Explain why alternatives are unsuitable."
        ],
        "evidence_types": [
          "Standards bodies",
          "Statutes/regulations",
          "Scholarly consensus"
        ],
        "answer_templates": [
          "The chosen definition is context‑appropriate and authoritative."
        ],
        "pitfalls": [
          "Non‑authoritative or cherry‑picked definitions"
        ]
      },
      {
        "id": "borderline",
        "title": "Borderline cases",
        "question": "How are gray areas resolved?",
        "short": "Define thresholds or tie‑breakers for near‑miss cases.",
        "why_it_matters": "Decisions often hinge on gray areas; make the rule explicit.",
        "how_to_answer": [
          "Specify thresholds and tie‑breakers, citing precedent or guidance.",
          "Define thresholds for close cases.",
          "Cite precedent or policy for gray‑area resolution."
        ],
        "evidence_types": [
          "Precedents",
          "Guidance memos",
          "Calibration protocols"
        ],
        "answer_templates": [
          "Gray‑area handling is explicit and defensible."
        ],
        "pitfalls": [
          "Silent thresholds",
          "Inconsistent handling of edge cases"
        ]
      }
    ]
  },
  "analogy": {
    "name": "Argument from Analogy",
    "description": "Infer judgment for a target case by similarity to a base case.",
    "critical_questions": [
      {
        "id": "similarities",
        "title": "Relevant similarities",
        "question": "What material features match between the cases?",
        "short": "List shared, decision‑relevant features that carry the judgment across.",
        "why_it_matters": "Only relevant similarities support transfer.",
        "how_to_answer": [
          "Enumerate shared, decision‑relevant features and why they matter for the outcome.",
          "Enumerate shared features that determine the outcome in both cases.",
          "Justify why those features are decisive."
        ],
        "evidence_types": [
          "Feature tables",
          "Domain theory",
          "Precedents"
        ],
        "answer_templates": [
          "The analogy rests on features that are material to the outcome."
        ],
        "pitfalls": [
          "Superficial similarities",
          "Irrelevant attributes"
        ]
      },
      {
        "id": "relevant_differences",
        "title": "Relevant differences",
        "question": "What breaks or limits the analogy?",
        "short": "Own the disanalogies and explain whether they defeat the analogy.",
        "why_it_matters": "A decisive difference can void the inference.",
        "how_to_answer": [
          "List differences and assess whether they are decisive; state limits to transfer.",
          "List differences and assess decisiveness.",
          "Explain limits where transfer does not hold."
        ],
        "evidence_types": [
          "Counter‑examples",
          "Sensitivity arguments",
          "Precedents"
        ],
        "answer_templates": [
          "The argument acknowledges disanalogies and justifies transfer limits."
        ],
        "pitfalls": [
          "Hiding disanalogies",
          "Overclaiming generality"
        ]
      },
      {
        "id": "analogy_base_rate",
        "title": "Typicality",
        "question": "Are both cases representative rather than outliers?",
        "short": "Show base and target are typical of the class.",
        "why_it_matters": "Outliers make analogies fragile.",
        "how_to_answer": [
          "Provide base rates or distributional context; show both cases are typical on key features.",
          "Provide base rates and distributional context.",
          "Show both cases are not extreme on key dimensions."
        ],
        "evidence_types": [
          "Descriptive statistics",
          "Surveys",
          "Benchmarks"
        ],
        "answer_templates": [
          "Both cases are typical members of the relevant class on key dimensions."
        ],
        "pitfalls": [
          "Cherry‑picked exemplar",
          "Survivorship bias"
        ]
      }
    ]
  },
  "cause_to_effect": {
    "name": "Cause to Effect",
    "description": "Argue that a proposed cause will likely produce an effect.",
    "critical_questions": [
      {
        "id": "strength",
        "title": "Causal strength",
        "question": "How large and consistent is the effect?",
        "short": "Report effect sizes with uncertainty; show consistency across settings.",
        "why_it_matters": "Magnitude and reliability determine practical importance.",
        "how_to_answer": [
          "Report effect sizes with intervals and demonstrate consistency across settings.",
          "Show replication or heterogeneity analysis.",
          "Provide uncertainty intervals."
        ],
        "evidence_types": [
          "RCTs/Quasi‑experiments",
          "Meta‑analyses",
          "Panel/event studies"
        ],
        "answer_templates": [
          "Effects of the cause are consistently observed with quantified uncertainty."
        ],
        "pitfalls": [
          "No uncertainty",
          "Single‑study cherry picking"
        ]
      },
      {
        "id": "mechanism",
        "title": "Mechanism",
        "question": "What process links cause to effect?",
        "short": "Describe the causal pathway and intermediate variables; connect to evidence.",
        "why_it_matters": "Plausible mechanisms increase confidence and guide design.",
        "how_to_answer": [
          "Describe the causal chain and provide evidence for each link.",
          "Describe the causal chain and mediators.",
          "Provide evidence for mediator links."
        ],
        "evidence_types": [
          "Mediation analyses",
          "Lab/field studies",
          "Process tracing"
        ],
        "answer_templates": [
          "The causal pathway is described and supported by evidence at key links."
        ],
        "pitfalls": [
          "Vague narratives",
          "Timing contradictions"
        ]
      },
      {
        "id": "temporality",
        "title": "Temporality",
        "question": "Does the cause precede the effect?",
        "short": "Show correct time order and rule out reverse causality.",
        "why_it_matters": "Causation requires correct time order.",
        "how_to_answer": [
          "Establish time order and rule out reverse causality risks.",
          "Use event timing/lag structures or experimental design.",
          "Show robustness to alternative lags."
        ],
        "evidence_types": [
          "Event studies",
          "Lag regressions",
          "Instrumental designs"
        ],
        "answer_templates": [
          "Timing evidence supports the claimed direction of effect."
        ],
        "pitfalls": [
          "Pretrend violations",
          "Contemporaneous-only data"
        ]
      },
      {
        "id": "confounders",
        "title": "Confounders",
        "question": "Are important common causes controlled?",
        "short": "Address major backdoors via design or controls; show robustness.",
        "why_it_matters": "Uncontrolled confounding can create spurious effects.",
        "how_to_answer": [
          "List key confounders and control strategy; provide robustness/sensitivity checks.",
          "List confounders and the identification strategy.",
          "Provide robustness checks."
        ],
        "evidence_types": [
          "Design papers",
          "Sensitivity analyses",
          "Balance tables"
        ],
        "answer_templates": [
          "Confounding is addressed; results robust to reasonable alternatives."
        ],
        "pitfalls": [
          "Superficial over‑control",
          "No robustness checks"
        ]
      },
      {
        "id": "interveners",
        "title": "Interveners",
        "question": "What enabling conditions and blockers affect the effect?",
        "short": "List enabling conditions and blockers; show they hold or are mitigated.",
        "why_it_matters": "Causal effects are often contingent on context.",
        "how_to_answer": [
          "List necessary conditions and likely blockers; demonstrate they hold or are mitigated.",
          "Verify necessary conditions.",
          "Identify blockers and show absence or mitigation."
        ],
        "evidence_types": [
          "Process evaluations",
          "Context analyses",
          "Readiness assessments"
        ],
        "answer_templates": [
          "Enablers are present and blockers are addressed."
        ],
        "pitfalls": [
          "Assuming universality",
          "Ignoring context"
        ]
      },
      {
        "id": "robustness",
        "title": "Robustness",
        "question": "Do findings replicate across methods or datasets?",
        "short": "Provide replications or triangulation across designs and samples.",
        "why_it_matters": "Replicability guards against fragile results.",
        "how_to_answer": [
          "Show similar effects across designs/samples; explain differences.",
          "Replicate across datasets/specs; triangulate with other evidence."
        ],
        "evidence_types": [
          "Multi‑study syntheses",
          "Cross‑validation",
          "Registered replications"
        ],
        "answer_templates": [
          "Results replicate across methods and datasets or differences are explained."
        ],
        "pitfalls": [
          "One‑off results",
          "No out‑of‑sample validation"
        ]
      }
    ]
  },
  "correlation_to_causation": {
    "name": "Correlation to Causation",
    "description": "Argue from an observed association to a causal claim.",
    "critical_questions": [
      {
        "id": "temporality",
        "title": "Temporality",
        "question": "Does the exposure precede the outcome?",
        "short": "Establish time order and rule out reverse causality.",
        "why_it_matters": "Causation requires correct time order.",
        "how_to_answer": [
          "Establish correct time order and show why reverse causality is unlikely.",
          "Use event timing or lag structures.",
          "Show robustness to alternative lags."
        ],
        "evidence_types": [
          "Event studies",
          "Lag models"
        ],
        "answer_templates": [
          "Time order supports the hypothesized causal direction."
        ],
        "pitfalls": [
          "Reverse causality risk",
          "No pre‑trend check"
        ]
      },
      {
        "id": "confounders",
        "title": "Confounders",
        "question": "Could common causes explain the association?",
        "short": "Control or design away important common causes.",
        "why_it_matters": "Uncontrolled confounding can create spurious associations.",
        "how_to_answer": [
          "List major common causes and explain the control/design strategy.",
          "Explain the control/design strategy.",
          "Provide robustness checks."
        ],
        "evidence_types": [
          "Matching/IV/DiD",
          "Sensitivity analyses"
        ],
        "answer_templates": [
          "Association persists after addressing common causes."
        ],
        "pitfalls": [
          "Omitted variables",
          "No robustness"
        ]
      },
      {
        "id": "mechanism",
        "title": "Mechanism",
        "question": "By what process does the exposure influence the outcome?",
        "short": "Provide a plausible pathway with supporting evidence.",
        "why_it_matters": "Mechanism increases credibility and external validity.",
        "how_to_answer": [
          "Describe a plausible pathway and provide supporting evidence.",
          "Describe pathway and testable mediators.",
          "Provide evidence for mediators."
        ],
        "evidence_types": [
          "Mediation/process studies"
        ],
        "answer_templates": [
          "A plausible pathway is articulated and supported."
        ],
        "pitfalls": [
          "Vague mechanism"
        ]
      },
      {
        "id": "robustness",
        "title": "Robustness",
        "question": "Does the association survive checks and replications?",
        "short": "Replicate across datasets/specs; run sensitivity tests.",
        "why_it_matters": "Robustness distinguishes real effects from artifacts.",
        "how_to_answer": [
          "Replicate across datasets/specs; provide sensitivity/bias analyses.",
          "Replicate in other data; vary specs.",
          "Run sensitivity/bias analyses."
        ],
        "evidence_types": [
          "Replication studies",
          "Sensitivity analyses"
        ],
        "answer_templates": [
          "Association remains in replications and sensitivity checks."
        ],
        "pitfalls": [
          "Single dataset only"
        ]
      }
    ]
  },
  "expert_opinion": {
    "name": "Expert Opinion",
    "description": "Rely on the judgment of a qualified expert or panel.",
    "critical_questions": [
      {
        "id": "credibility",
        "title": "Credibility",
        "question": "Is the source genuinely expert?",
        "short": "Establish qualifications, experience, and track record.",
        "why_it_matters": "Source quality drives trust in testimony.",
        "how_to_answer": [
          "Summarize qualifications and evidence of accuracy over time.",
          "List degrees/experience/appointments.",
          "Show calibration or past accuracy."
        ],
        "evidence_types": [
          "CVs",
          "Peer recognition",
          "Calibration studies"
        ],
        "answer_templates": [
          "The source is qualified and has demonstrated accuracy or sound judgment."
        ],
        "pitfalls": [
          "Appeal to fame rather than expertise"
        ]
      },
      {
        "id": "domain_fit",
        "title": "Domain fit",
        "question": "Does the expertise match the claim?",
        "short": "Align the domain/scope of expertise with the claim; note limits.",
        "why_it_matters": "Out‑of‑domain claims carry less weight.",
        "how_to_answer": [
          "Map the claim facets to the expert’s domain; note cross‑disciplinary limits.",
          "Map claim facets to domain boundaries.",
          "Note cross‑disciplinary limits."
        ],
        "evidence_types": [
          "Domain maps",
          "Scope notes"
        ],
        "answer_templates": [
          "The claim falls within the expert’s domain (with caveats for any parts that do not)."
        ],
        "pitfalls": [
          "Assuming expertise transfers everywhere"
        ]
      },
      {
        "id": "reliability",
        "title": "Reliability",
        "question": "How accurate is the source generally?",
        "short": "Provide calibration, error rates, or audit results.",
        "why_it_matters": "Past performance predicts current credibility.",
        "how_to_answer": [
          "Provide calibration, error rates, or audits; note incentives for honesty.",
          "Provide calibration metrics, error rates, or audits.",
          "Note incentives for honesty."
        ],
        "evidence_types": [
          "Calibration studies",
          "Audits"
        ],
        "answer_templates": [
          "Evidence indicates the source is generally accurate and appropriately incentivized."
        ],
        "pitfalls": [
          "No evidence of accuracy"
        ]
      },
      {
        "id": "bias",
        "title": "Bias",
        "question": "Any conflicts of interest?",
        "short": "Disclose conflicts and explain mitigation or independence.",
        "why_it_matters": "Undisclosed conflicts can skew testimony.",
        "how_to_answer": [
          "List interests/funding and explain how conflicts are mitigated.",
          "List interests and mitigation measures."
        ],
        "evidence_types": [
          "COI statements",
          "Funding records"
        ],
        "answer_templates": [
          "Conflicts are disclosed and mitigated or independence is established."
        ],
        "pitfalls": [
          "No disclosure"
        ]
      },
      {
        "id": "corroboration",
        "title": "Corroboration",
        "question": "Are there independent confirmations?",
        "short": "Cite convergent evidence from independent sources.",
        "why_it_matters": "Independent lines of evidence increase confidence.",
        "how_to_answer": [
          "Provide independent replications or corroborating sources.",
          "Provide independent replications/observations."
        ],
        "evidence_types": [
          "Replication/triangulation"
        ],
        "answer_templates": [
          "Independent sources converge on the same conclusion."
        ],
        "pitfalls": [
          "Single-source reliance"
        ]
      }
    ]
  },
  "position_to_know": {
    "name": "Position to Know (Testimony)",
    "description": "Trust a witness/source with privileged access to facts.",
    "critical_questions": [
      {
        "id": "access",
        "title": "Access",
        "question": "Did the source have the right information?",
        "short": "Explain access, proximity, or records.",
        "why_it_matters": "Without access, testimony is weak.",
        "how_to_answer": [
          "Describe how/why the source had access to the facts.",
          "Describe the access channel (role, system, records)."
        ],
        "evidence_types": [
          "Access logs",
          "Records",
          "Proximity evidence"
        ],
        "answer_templates": [
          "Access pathway is clear and credible."
        ],
        "pitfalls": [
          "Speculative access"
        ]
      },
      {
        "id": "reliability",
        "title": "Reliability",
        "question": "Is the source generally accurate?",
        "short": "Provide evidence of accuracy and incentives for honesty.",
        "why_it_matters": "Accuracy and incentives shape trust.",
        "how_to_answer": [
          "Provide calibration or audits and note incentives for honesty.",
          "Provide calibration or audit evidence; note incentives to be truthful."
        ],
        "evidence_types": [
          "Calibration/audits",
          "Incentive structures"
        ],
        "answer_templates": [
          "The source is generally accurate and appropriately incentivized."
        ],
        "pitfalls": [
          "No accuracy record"
        ]
      },
      {
        "id": "bias",
        "title": "Bias",
        "question": "Could interests distort the statement?",
        "short": "Disclose and manage conflicts of interest.",
        "why_it_matters": "Conflicts can skew testimony.",
        "how_to_answer": [
          "List material interests and describe mitigation measures.",
          "List interests and mitigation measures."
        ],
        "evidence_types": [
          "COI statements",
          "Firewalls"
        ],
        "answer_templates": [
          "Material conflicts are disclosed and mitigated."
        ],
        "pitfalls": [
          "Undisclosed COIs"
        ]
      },
      {
        "id": "corroboration",
        "title": "Corroboration",
        "question": "Do other sources agree?",
        "short": "Provide independent confirmations.",
        "why_it_matters": "Convergent evidence strengthens testimony.",
        "how_to_answer": [
          "Provide independent confirmations from credible sources.",
          "Cite independent records or witnesses."
        ],
        "evidence_types": [
          "Independent records",
          "Multiple witnesses"
        ],
        "answer_templates": [
          "Independent confirmations are present."
        ],
        "pitfalls": [
          "No independent confirmation"
        ]
      }
    ]
  },
  "example": {
    "name": "Argument from Example",
    "description": "Generalize from one or more cases to a broader claim.",
    "critical_questions": [
      {
        "id": "sample_size",
        "title": "Sample size",
        "question": "Is the number of cases sufficient to support the generalization?",
        "short": "State the number of cases and justify that it is sufficient for precision/power.",
        "why_it_matters": "Too few cases make generalization unreliable.",
        "how_to_answer": [
          "Report the number of cases and expected precision; justify adequacy.",
          "Report N and expected precision.",
          "Justify adequacy for the use case."
        ],
        "evidence_types": [
          "Power calculations",
          "Precision estimates"
        ],
        "answer_templates": [
          "The number of cases is sufficient for the claimed precision."
        ],
        "pitfalls": [
          "No justification of N"
        ]
      },
      {
        "id": "representativeness",
        "title": "Representativeness",
        "question": "Do the observed cases reflect the target population?",
        "short": "Describe sampling frame and selection; note differences.",
        "why_it_matters": "Biased samples mislead.",
        "how_to_answer": [
          "Describe sampling and compare to the population on key features.",
          "Explain selection and compare on key features."
        ],
        "evidence_types": [
          "Sampling frames",
          "Comparability tables"
        ],
        "answer_templates": [
          "The sample reflects the population on decision‑relevant features."
        ],
        "pitfalls": [
          "Convenience samples without caveats"
        ]
      },
      {
        "id": "defeaters",
        "title": "Defeaters",
        "question": "Are there known exceptions that would defeat the generalization?",
        "short": "Name exceptions and why they do not apply here.",
        "why_it_matters": "Exceptions can overturn inductive claims.",
        "how_to_answer": [
          "List plausible exceptions and argue non‑applicability or bound their impact.",
          "List plausible exceptions; argue non‑applicability or bound impact."
        ],
        "evidence_types": [
          "Counterexamples",
          "Boundary conditions"
        ],
        "answer_templates": [
          "Plausible exceptions are addressed or bounded."
        ],
        "pitfalls": [
          "Ignoring known exceptions"
        ]
      }
    ]
  },
  "sign": {
    "name": "Argument from Sign",
    "description": "Infer a hidden state from an observed sign or indicator.",
    "critical_questions": [
      {
        "id": "sign",
        "title": "Sign reliability",
        "question": "Does the sign reliably indicate the state?",
        "short": "Report diagnostic accuracy (sensitivity/specificity) or predictive value.",
        "why_it_matters": "Unreliable signs misclassify states.",
        "how_to_answer": [
          "Provide diagnostic accuracy metrics and threshold calibration.",
          "Provide sensitivity/specificity or predictive values.",
          "Calibrate thresholds."
        ],
        "evidence_types": [
          "Diagnostic studies",
          "Calibration reports"
        ],
        "answer_templates": [
          "Diagnostic performance is adequate for the decision context."
        ],
        "pitfalls": [
          "No diagnostic metrics"
        ]
      },
      {
        "id": "alt_explanations",
        "title": "Alternative explanations",
        "question": "Could the sign be caused by something else?",
        "short": "Rule out plausible rival causes.",
        "why_it_matters": "Ambiguity reduces inferential value.",
        "how_to_answer": [
          "List plausible alternative causes and rule out or bound them.",
          "List plausible alternatives; test and rule out or bound them."
        ],
        "evidence_types": [
          "Causal graphs",
          "Sensitivity analyses"
        ],
        "answer_templates": [
          "Plausible rival causes have been addressed."
        ],
        "pitfalls": [
          "Ignoring plausible rivals"
        ]
      },
      {
        "id": "base_rate",
        "title": "Base rate",
        "question": "How common is the state in the target population?",
        "short": "Provide base rates; avoid base‑rate fallacy.",
        "why_it_matters": "Base rates change predictive value.",
        "how_to_answer": [
          "Provide baseline prevalence for the target population and explain implications.",
          "State baseline prevalence for the target population."
        ],
        "evidence_types": [
          "Prevalence studies",
          "Registries"
        ],
        "answer_templates": [
          "Baseline prevalence is stated and used to calibrate predictive value."
        ],
        "pitfalls": [
          "Using non‑comparable base rate"
        ]
      },
      {
        "id": "measurement",
        "title": "Measurement",
        "question": "Is the observation measured reliably?",
        "short": "Show instrument quality and inter‑rater reliability.",
        "why_it_matters": "Measurement error can flip conclusions.",
        "how_to_answer": [
          "Provide reliability metrics and quality‑assurance procedures.",
          "Provide reliability metrics and QA procedures."
        ],
        "evidence_types": [
          "Reliability studies",
          "QA logs"
        ],
        "answer_templates": [
          "Measurement reliability is documented."
        ],
        "pitfalls": [
          "No QA evidence"
        ]
      }
    ]
  }
}